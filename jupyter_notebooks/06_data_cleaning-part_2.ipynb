{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Zweiter Teil: Unnötige Daten und Inkonsistenzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wann können Daten unnötig sein?\n",
    "* Uninformativ/Wiederholungen\n",
    "* Irrelevant\n",
    "* Duplikate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Uninformative Daten\n",
    "\n",
    "Features können uninformativ sein, wenn viele Einträge identisch sind. Auch hier gibt es wieder keine allgemeine Regel, sondern es hängt von den Umständen ab.\n",
    "\n",
    "Wir erkunden den Datensatz mit der willkürlichen Grenze 95% identischer Daten pro Feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oil_chemistry_raion: 99.02858%\n",
      "no     30175\n",
      "yes      296\n",
      "Name: oil_chemistry_raion, dtype: int64\n",
      "\n",
      "railroad_terminal_raion: 96.27187%\n",
      "no     29335\n",
      "yes     1136\n",
      "Name: railroad_terminal_raion, dtype: int64\n",
      "\n",
      "nuclear_reactor_raion: 97.16780%\n",
      "no     29608\n",
      "yes      863\n",
      "Name: nuclear_reactor_raion, dtype: int64\n",
      "\n",
      "big_road1_1line: 97.43691%\n",
      "no     29690\n",
      "yes      781\n",
      "Name: big_road1_1line, dtype: int64\n",
      "\n",
      "railroad_1line: 97.06934%\n",
      "no     29578\n",
      "yes      893\n",
      "Name: railroad_1line, dtype: int64\n",
      "\n",
      "cafe_count_500_price_high: 97.25641%\n",
      "0    29635\n",
      "1      787\n",
      "2       38\n",
      "3       11\n",
      "Name: cafe_count_500_price_high, dtype: int64\n",
      "\n",
      "mosque_count_500: 99.51101%\n",
      "0    30322\n",
      "1      149\n",
      "Name: mosque_count_500, dtype: int64\n",
      "\n",
      "cafe_count_1000_price_high: 95.52689%\n",
      "0    29108\n",
      "1     1104\n",
      "2      145\n",
      "3       51\n",
      "4       39\n",
      "5       15\n",
      "6        8\n",
      "7        1\n",
      "Name: cafe_count_1000_price_high, dtype: int64\n",
      "\n",
      "mosque_count_1000: 98.08342%\n",
      "0    29887\n",
      "1      584\n",
      "Name: mosque_count_1000, dtype: int64\n",
      "\n",
      "mosque_count_1500: 96.21936%\n",
      "0    29319\n",
      "1     1152\n",
      "Name: mosque_count_1500, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/sberbank-russian-housing-market/train.csv\")\n",
    "\n",
    "num_rows = len(df.index)\n",
    "low_information_cols = []\n",
    "\n",
    "for col in df.columns:\n",
    "    cnts = df[col].value_counts(dropna=False)\n",
    "    top_pct = (cnts/num_rows).iloc[0]\n",
    "\n",
    "    if top_pct > 0.95:\n",
    "        low_information_cols.append(col)\n",
    "        print('{0}: {1:.5f}%'.format(col, top_pct*100))\n",
    "        print(cnts)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Was ist zu tun?\n",
    "\n",
    "Uninformative Features können ausgeschlossen (gelöscht) werden. Dies sollte aber nicht leichtfertig geschehen.\n",
    "Im Beispiel würde ich per se keins der Features ausschliessen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Irrelevante Daten\n",
    "\n",
    "Dies ist eher eine qualitative Aufgabe, die Daten selber helfen uns dabei nur bedingt. Irrelevante Daten _können_ später im Rahmen der Datenanalyse ermittelt werden, also Daten, die beispielsweise für ein Machine-Learning-Modell keine Rolle spielen. Das wissen wir aber ja zum Zeitpunkt des Cleanings noch nicht!\n",
    "\n",
    "Trotzdem können irrelevante Daten auch in der Praxis auftreten, beispielsweise bei der Kombination von Datensätzen unterschiedlicher Herkunft. Daten, die in einem Anwendungsfeld relevant sind, müssen dies nicht auch in anderen Feldern sein.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Duplikate\n",
    "\n",
    "Duplikate können in Zeilen und Spalten auftreten. Bei der Suche nach Duplikaten eindeutige IDs usw. ausschliessen.\n",
    "\n",
    "Wir unterscheiden weiter zwischen vollständigen Duplikaten (Kopien) und solchen, die sich nur wenig unterscheiden - also analog zu den uninformativen Daten weiter oben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30471, 292)\n",
      "(30461, 291)\n"
     ]
    }
   ],
   "source": [
    "# Spalte \"id\" muss ausgeschlossen werden, dann kann \"drop_duplicates\" genutzt werden\n",
    "df_dedupped = df.drop('id', axis=1).drop_duplicates()\n",
    "\n",
    "# falls was fehlt, waren es Duplikate...\n",
    "print(df.shape)\n",
    "print(df_dedupped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Eine Spalte fehlt - das waren wir selbst (id).\n",
    "Zehn Zeilen fehlen, das waren Duplikate.\n",
    "\n",
    "Damit sind die Duplikate auch gleich gelöscht..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Auch sich leicht unterscheidende Dateneinträge (Zeilen) können Duplikate sein.\n",
    "Wie kann so etwas passieren? Beispielsweise doppelte manuelle Einträge. Wer schon mal eine Wohnung oder ein Haus gesucht hat, weiss, dass es oft identische Objekte mit leicht abgewandelten Beschreibungen auf unterschiedlichen Portalen gibt. So können über automatische oder manuelle Datenerfassung auch sich leicht unterscheidende, aber eigentlich identische Objekte in die Datensammlung gelangen.\n",
    "\n",
    "Wir machen eine Probe: Gibt es Wohnungen mit:\n",
    "* identischem Zeitstempel\n",
    "* gleicher Grundfläche\n",
    "* gleichem Stockwerk und Raumzahl\n",
    "* gleichem Preis\n",
    "* gleichem Baujahr\n",
    "\n",
    "Dann gehen wir davon aus, dass dies Duplikate sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp   full_sq  life_sq  floor  build_year  num_room  price_doc\n",
       "2014-04-15  134       134.0   1.0     0.0         3.0      5798496      2\n",
       "2015-03-30  41        41.0    11.0    2016.0      1.0      4114580      2\n",
       "2015-03-14  62       -999.0   2.0    -999.0       2.0      6520500      2\n",
       "2012-09-05  43       -999.0   21.0   -999.0      -999.0    6229540      2\n",
       "2014-12-09  40       -999.0   17.0   -999.0       1.0      4607265      2\n",
       "2013-06-24  40       -999.0   12.0   -999.0      -999.0    4112800      2\n",
       "2013-08-30  40       -999.0   12.0   -999.0       1.0      4462000      2\n",
       "2013-09-23  85       -999.0   14.0   -999.0       3.0      7725974      2\n",
       "2012-08-27  59       -999.0   6.0    -999.0      -999.0    4506800      2\n",
       "2014-01-22  46        28.0    1.0     1968.0      2.0      3000000      2\n",
       "2012-10-22  61       -999.0   18.0   -999.0      -999.0    8248500      2\n",
       "2013-04-03  42       -999.0   2.0    -999.0      -999.0    3444000      2\n",
       "2014-12-17  62       -999.0   9.0    -999.0       2.0      6552000      2\n",
       "2013-05-22  68       -999.0   2.0    -999.0      -999.0    5406690      2\n",
       "2013-12-18  39       -999.0   6.0    -999.0       1.0      3700946      2\n",
       "2013-12-05  40       -999.0   5.0    -999.0       1.0      4414080      2\n",
       "2014-06-28  38       -999.0   6.0    -999.0       1.0      4737189      1\n",
       "            39        20.0    8.0     1980.0      1.0      7400000      1\n",
       "            38        23.0    14.0    1971.0      2.0      8000000      1\n",
       "                      19.0    4.0     1986.0      1.0      9000000      1\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = ['timestamp', 'full_sq', 'life_sq', 'floor', 'build_year', 'num_room', 'price_doc']\n",
    "\n",
    "df.fillna(-999).groupby(keys)['id'].count().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wie wir sehen, sind die ersten 16 Einträge (nach unserer Logik) Duplikate.\n",
    "\n",
    "Was kann man tun? Am besten die verdächtigen Daten genauer unter die Lupe nehmen und dann entscheiden, ob gelöscht werden soll:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30471, 292)\n",
      "(30455, 292)\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates based on a subset of variables.\n",
    "\n",
    "df_dedupped2 = df.drop_duplicates(subset=keys)\n",
    "\n",
    "print(df.shape)\n",
    "print(df_dedupped2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "16 Einträge weniger..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Inkonsistente Daten\n",
    "\n",
    "Daten-Inkonsistenzen sind heimtückischer als die fehlenden oder mehrfach vorhandenen Daten, die wir bislang behandelt haben. Der Datensatz sieht OK aus, Modelle und Analysen funktionieren prinzipiell, aber können nicht ihr volles Potenzial entfalten.\n",
    "\n",
    "Warum? Eigentlich identische Feature-Werte werden beispielsweise wegen abweichender Schreibweise als unterschiedlich gezählt. Andere Werte sind irrtümlich identisch, sollten aber eigentlich unterschiedlich sein, beispielsweise wegen abweichender Formate oder Rundung.\n",
    "\n",
    "Damit gibt es auch kein Standard-Rezept, um alle Inkonsistenzen aufzuspüren. Hier werden einige Beispiele vorgeführt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Inkonsistente Schreibweise\n",
    "#### Häufiges Beispiel: Gross-/Kleinschreibung\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Poselenie Sosenskoe               1776\n",
       "Nekrasovka                        1611\n",
       "Poselenie Vnukovskoe              1372\n",
       "Poselenie Moskovskij               925\n",
       "Poselenie Voskresenskoe            713\n",
       "                                  ... \n",
       "Molzhaninovskoe                      3\n",
       "Poselenie Shhapovskoe                2\n",
       "Poselenie Kievskij                   2\n",
       "Poselenie Klenovskoe                 1\n",
       "Poselenie Mihajlovo-Jarcevskoe       1\n",
       "Name: sub_area, Length: 146, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sub_area'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hier scheint kein Problem vorzuliegen.\n",
    "Man kann sich aber leicht vorstellen, dass unterschiedliche Schreibweisen der Regionen vorkommen könnten.\n",
    "\n",
    "Wie können die aufgespürt werden? Mittels Normalisierung der Schreibweise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "poselenie sosenskoe               1776\n",
       "nekrasovka                        1611\n",
       "poselenie vnukovskoe              1372\n",
       "poselenie moskovskij               925\n",
       "poselenie voskresenskoe            713\n",
       "                                  ... \n",
       "molzhaninovskoe                      3\n",
       "poselenie shhapovskoe                2\n",
       "poselenie kievskij                   2\n",
       "poselenie klenovskoe                 1\n",
       "poselenie mihajlovo-jarcevskoe       1\n",
       "Name: sub_area_lower, Length: 146, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sub_area_lower'] = df['sub_area'].str.lower()  # neue Spalte anlegen\n",
    "df['sub_area_lower'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(keine Veränderung zu vorher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Datenformate\n",
    "\n",
    "Häufig auftretendes Beispiel: Datumsformate\n",
    "Sind oft Strings, keine Datumswerte. Manuelle Umwandlung ist notwendig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 30471 entries, 0 to 30470\n",
      "Series name: timestamp\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "30471 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 238.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df[\"timestamp\"].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hier: object, also eigentlich ein String.\n",
    "\n",
    "Eine Umwandlung hilft uns doppelt. Zum einen wird geprüft, ob alle Einträge gültige Datumswerte sind. Zum anderen können wir hinterher einfacher auf Zeitbereiche zugreifen (Jahre, Quartale usw.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014    13662\n",
      "2013     7978\n",
      "2012     4839\n",
      "2015     3239\n",
      "2011      753\n",
      "Name: year, dtype: int64\n",
      "\n",
      "12    3400\n",
      "4     3191\n",
      "3     2972\n",
      "11    2970\n",
      "10    2736\n",
      "6     2570\n",
      "5     2496\n",
      "9     2346\n",
      "2     2275\n",
      "7     1875\n",
      "8     1831\n",
      "1     1809\n",
      "Name: month, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['timestamp_dt'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d')  # explizite Formatangabe: 4 Ziffern Jahr, je 2 für Monat und Tag\n",
    "df['year'] = df['timestamp_dt'].dt.year  # Explizite Speicherung eigentlich nicht notwendig, hier nur zur Verdeutlichung\n",
    "df['month'] = df['timestamp_dt'].dt.month\n",
    "df['weekday'] = df['timestamp_dt'].dt.weekday\n",
    "\n",
    "print(df['year'].value_counts(dropna=False))\n",
    "print()\n",
    "print(df['month'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 30471 entries, 0 to 30470\n",
      "Series name: timestamp_dt\n",
      "Non-Null Count  Dtype         \n",
      "--------------  -----         \n",
      "30471 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1)\n",
      "memory usage: 238.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.timestamp_dt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Kategorielle Daten\n",
    "\n",
    "Kategorielle Daten sollten eigentlich unproblematisch sein, da sie einen Wert aus einer vorgegebenen Anzahl Kategorien einnehmen. Beispiel: PKW, LKW usw. Trotzdem können durch Fehler bei der Datenerfassung oder bei der Datenzusammenführung Fehler auftreten, die bereinigt werden müssen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Was kann getan werden? Die Spalten mit kategoriellen Daten können untersucht werden, ob alle Werte aus den vorgegebenen Kategorien stammen. (Ohne Beispiel, im Datensatz gibt es dieses Problem nicht.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Weitere textuelle Daten wie Adressen usw.\n",
    "\n",
    "Bei Texten allgemein und auch bei speziellen Texten wie beispielsweise Adresswerten treten oft Probleme auf, die auf mangelnder Standardisierung beruhen. Dies lässt sich mit Methoden des NLP beheben. Das kommt dann nächstes Jahr..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Inhalte teilweise übernommen aus: https://towardsdatascience.com/data-cleaning-in-python-the-ultimate-guide-2020-c63b88bf0a0d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "511f98252977769337e6ede8befacc6974cba65790423b61aaa108af8e2092b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
